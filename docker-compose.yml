version: '3.8'

services:
  api:
    build: .
    container_name: web_crawler_rag_api
    ports:
      - "8000:8000"
    environment:
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - DEBUG=False
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY}
      - DATABASE_URL=sqlite:///./data/crawler_rag.db
      # Resource Limiting
      - MAX_WORKERS=2
      - CRAWLER_CONCURRENT_REQUESTS=4
      - CRAWLER_MAX_THREADS=4
      - MAX_EMBEDDING_BATCH_SIZE=32
      - CHROMADB_MAX_BATCH_SIZE=100
      - OMP_NUM_THREADS=4
      - OPENBLAS_NUM_THREADS=4
      - MKL_NUM_THREADS=4
      - VECLIB_MAXIMUM_THREADS=4
      - NUMEXPR_NUM_THREADS=4
    volumes:
      - ./data:/app/data
      - ./config:/app/config
    restart: unless-stopped
    # Docker resource limits
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 512M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Optional: Add PostgreSQL for production
  # postgres:
  #   image: postgres:15-alpine
  #   container_name: web_crawler_rag_db
  #   environment:
  #     - POSTGRES_DB=crawler_rag
  #     - POSTGRES_USER=crawler
  #     - POSTGRES_PASSWORD=your_secure_password
  #   volumes:
  #     - postgres_data:/var/lib/postgresql/data
  #   ports:
  #     - "5432:5432"
  #   restart: unless-stopped

  # Optional: Add Redis for caching
  # redis:
  #   image: redis:7-alpine
  #   container_name: web_crawler_rag_redis
  #   ports:
  #     - "6379:6379"
  #   restart: unless-stopped

volumes:
  postgres_data:
